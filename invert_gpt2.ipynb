{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ben_utils import SaveForward\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import torch as t\n",
    "import torchtext\n",
    "from tqdm import tqdm\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "devices = [f'cuda:{i}' for i in [2,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2 = transformers.GPT2LMHeadModel.from_pretrained('gpt2').to(devices[0]).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeheadedGPT2(t.nn.Module):\n",
    "    def __init__(self, gpt2, n_layer=None):\n",
    "        super().__init__()\n",
    "        if n_layer is None:\n",
    "            n_layer = gpt2.config.n_layer\n",
    "        self.blocks = t.nn.ModuleList(\n",
    "            [transformers.models.gpt2.modeling_gpt2.GPT2Block(gpt2.config) for _ in range(n_layer)]\n",
    "        )\n",
    "        self.ln_f = t.nn.LayerNorm((gpt2.config.n_embd,), eps=1e-05, elementwise_affine=True)\n",
    "        self.lm_head = t.nn.Linear(in_features=gpt2.config.n_embd, out_features=gpt2.config.vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for block in self.blocks:\n",
    "            (x,) = block(x)\n",
    "        x = self.ln_f(x)\n",
    "        x = self.lm_head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([31373,   612, 50256,  4919,   389,   345]),\n",
       "  'hello there<|endoftext|>how are you'),\n",
       " (tensor([ 1804, 50256,  1456,   318,   617,  6291]),\n",
       "  ' doing<|endoftext|>here is some sample')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def batches_from_data(data, max_seq_len):\n",
    "    assert max_seq_len > 0\n",
    "    batch_list = []\n",
    "    next_batch = t.zeros(max_seq_len, dtype=t.long)\n",
    "    remaining_space_in_next_batch = max_seq_len\n",
    "    for item in data:\n",
    "        tokens = tokenizer.encode(item, return_tensors=\"pt\").squeeze(0)\n",
    "        while len(tokens):\n",
    "            if remaining_space_in_next_batch <= 0:\n",
    "                batch_list.append(next_batch)\n",
    "                next_batch = t.zeros(max_seq_len, dtype=t.long)\n",
    "                remaining_space_in_next_batch = max_seq_len\n",
    "            n = remaining_space_in_next_batch\n",
    "            tokens_left = len(tokens)\n",
    "            for_this_batch, tokens = tokens[:n], tokens[n:]\n",
    "            assert for_this_batch.shape == (min(n, tokens_left),), (n, tokens_left, tokens, for_this_batch)\n",
    "            write_start = max_seq_len - remaining_space_in_next_batch\n",
    "            write_end = write_start + len(for_this_batch)\n",
    "            next_batch[write_start:write_end] = for_this_batch\n",
    "            remaining_space_in_next_batch -= len(for_this_batch)\n",
    "            if write_end < max_seq_len and remaining_space_in_next_batch > 0:\n",
    "                next_batch[write_end] = tokenizer.eos_token_id\n",
    "                remaining_space_in_next_batch -= 1\n",
    "    return batch_list\n",
    "\n",
    "[\n",
    "    (batch, tokenizer.decode(batch))\n",
    "    for batch in batches_from_data(\n",
    "            ['hello there', 'how are you doing', 'here is some sample data'],\n",
    "            max_seq_len=6,\n",
    "        )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data, batch_size, max_seq_len):\n",
    "    return t.utils.data.DataLoader(batches_from_data(data, max_seq_len=max_seq_len), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "def load_dataset(**kwargs):\n",
    "    data_train, data_test = torchtext.datasets.WikiText2(root='.data', split=('train', 'test'))\n",
    "    return load_data(data_train, **kwargs), load_data(data_test, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = load_dataset(batch_size=1, max_seq_len=gpt2.config.n_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(beheaded, dataloader):\n",
    "    layer_input_to_invert = gpt2.transformer.h[0]\n",
    "    optimizer = t.optim.Adam(beheaded.parameters())\n",
    "    logged_losses = []\n",
    "\n",
    "    for i, input_ids in enumerate(dataloader):\n",
    "        input_ids = input_ids.to(gpt2.device)\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        with SaveForward(layer_input_to_invert) as saved:\n",
    "            with t.no_grad():\n",
    "                gpt2(input_ids)\n",
    "            (to_invert,) = saved.saved_input\n",
    "            assert to_invert.shape == (batch_size, seq_len, gpt2.config.n_embd)\n",
    "        output_logits = beheaded(to_invert)\n",
    "        assert output_logits.shape == (batch_size, seq_len, tokenizer.vocab_size)\n",
    "        output_logits = einops.rearrange(output_logits, 'b i w -> (b i) w')\n",
    "        assert output_logits.shape == (batch_size * seq_len, tokenizer.vocab_size)\n",
    "        target = einops.rearrange(input_ids, 'b i -> (b i)')\n",
    "        assert target.shape == (batch_size * seq_len,)\n",
    "        loss = t.nn.functional.cross_entropy(output_logits, target)\n",
    "        logged_losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    plt.scatter(range(len(logged_losses)), logged_losses)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "458"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-2294f958660f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbeheaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeheadedGPT2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpt2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_layer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpt2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeheaded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-27-b244abee0618>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(beheaded, dataloader)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mlogged_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "beheaded = BeheadedGPT2(gpt2, n_layer=2).to(gpt2.device).train()\n",
    "train(beheaded, dataloader=data_train)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
