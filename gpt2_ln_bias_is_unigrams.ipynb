{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if t.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = transformers.GPT2LMHeadModel.from_pretrained(\"gpt2\").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.GPT2TokenizerFast.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The bias term in the final LayerNorm encodes unigram statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_unigrams(gpt2, extra_tokens_to_show=None):\n",
    "    with t.no_grad():\n",
    "        unigrams = gpt2.lm_head(gpt2.transformer.ln_f.bias)\n",
    "        probabilities = t.softmax(unigrams, dim=-1)\n",
    "        if extra_tokens_to_show is None:\n",
    "            extra_tokens_to_show = []\n",
    "        tokens_to_show = list(t.sort(unigrams, descending=True).indices[:10]) + extra_tokens_to_show\n",
    "    return [(tokenizer.decode(token), f\"{probabilities[token].item():0.2}\") for token in tokens_to_show]\n",
    "\n",
    "summarize_unigrams(gpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These look like plausible unigram statistics, but that doesn't tell me if they're *exactly* unigram statistics. In fact we expect them to be a good low-rank approximation to unigram statistics, since we don't have enough dimensions in the bias to encode the correct numbers.\n",
    "\n",
    "So, the claim instead is something like \"everything the model knows about unigram statistics, it knows via this bias term\". I'm a bit confused about what exactly it would mean for this to be true, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt2_with_tweaked_token(token, tweak, gpt2: transformers.GPT2LMHeadModel = None):\n",
    "    if gpt2 is None:\n",
    "        gpt2 = transformers.GPT2LMHeadModel.from_pretrained(\"gpt2\").to(DEVICE)\n",
    "    unigrams = gpt2.lm_head(gpt2.transformer.ln_f.bias)\n",
    "    unigrams[token] += tweak\n",
    "    new_bias = t.linalg.lstsq(gpt2.lm_head.weight, unigrams).solution\n",
    "    assert new_bias.shape == gpt2.transformer.ln_f.bias.shape\n",
    "    gpt2.transformer.ln_f.bias = t.nn.Parameter(new_bias)\n",
    "    return gpt2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a cute way of adjusting unigram statistics, but the resulting generated text can often end up being degenerate.\n",
    "\n",
    "While the above uses the assumption that minimizing least squares error is a good way to avoid corrupting unigram statistics that we don't want to change, and this assumption is a bit suspicious, it doesn't affect how legit the claim about the bias being the model's whole knowledge of unigram statistics."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7190ff053c3d90cfb76522ce77adf250c9fcce715af1d5695ddeeaa88dc7dc4c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
