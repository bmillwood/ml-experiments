{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ben_utils import SaveForward\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import sys\n",
    "import torch as t\n",
    "from tqdm import tqdm\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "devices = [f'cuda:{i}' for i in [2,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2 = transformers.GPT2LMHeadModel.from_pretrained('gpt2').to(devices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_babynames_csv(path):\n",
    "    counts_by_name_by_gender = defaultdict(lambda: defaultdict(int))\n",
    "    with open(csv_name, newline='') as csv_file:\n",
    "        dict_reader = csv.DictReader(csv_file)\n",
    "        for row in dict_reader:\n",
    "            name = row['Name']\n",
    "            _year = int(row['Year'])\n",
    "            gender = row['Gender']\n",
    "            count = int(row['Count'])\n",
    "            counts_by_name_by_gender[gender][name] = count\n",
    "\n",
    "    return counts_by_name_by_gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_name = 'NationalNames.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['F', 'M'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from_babynames_csv(csv_name).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_embedding(gpt2, token):\n",
    "    assert token.shape == (1, 1, gpt2.config.n_embd)\n",
    "    with SaveForward(gpt2.lm_head) as saved:\n",
    "        gpt2(token)\n",
    "        (embedding,) = saved.saved_input\n",
    "\n",
    "    return embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_to_single_token_reps(tokenizer, words):\n",
    "    result = {}\n",
    "    for word in words:\n",
    "        tokens = tokenizer.encode(word, return_tensors=\"pt\")\n",
    "        batch_size, seq_len = tokens.shape\n",
    "        assert batch_size == 1\n",
    "        if seq_len == 1:\n",
    "            result[word] = tokens.item()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3443733 of 4088706 people (84%) do not have names with single-token representations\n",
      "Top three missing names: [('Emma', 20811), ('Olivia', 19696), ('Noah', 19250)]\n"
     ]
    }
   ],
   "source": [
    "def how_bad_is_restricting_to_single_tokens(tokenizer):\n",
    "    counts_by_name_by_gender = from_babynames_csv(csv_name)\n",
    "\n",
    "    counts_by_name = defaultdict(int)\n",
    "    for gender in ['M', 'F']:\n",
    "        for name, count in counts_by_name_by_gender[gender].items():\n",
    "            counts_by_name[name] += count\n",
    "    \n",
    "    with_single_token_reps = words_to_single_token_reps(tokenizer, counts_by_name).keys()\n",
    "    \n",
    "    counts_by_name_without_single_token_reps = counts_by_name.copy()\n",
    "    for name in with_single_token_reps:\n",
    "        del counts_by_name_without_single_token_reps[name]\n",
    "    \n",
    "    total_without = sum(counts_by_name_without_single_token_reps.values())\n",
    "    total = sum(counts_by_name.values())\n",
    "    print(f'{total_without} of {total} people ({total_without / total:.0%}) do not have names with single-token representations')\n",
    "    \n",
    "    sorted_by_incidence = list(counts_by_name_without_single_token_reps.items())\n",
    "    sorted_by_incidence.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(f'Top three missing names: {sorted_by_incidence[:3]}')\n",
    "\n",
    "how_bad_is_restricting_to_single_tokens(tokenizer)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
